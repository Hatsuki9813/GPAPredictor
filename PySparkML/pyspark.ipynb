{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       mssv  hocky  namhoc  dtbhk  sotchk  \\\n",
      "0  0000AC05XPvAibaEXe9B2tolTZ0JLoBGbkQixQS6    2.0  2021.0   8.28    21.0   \n",
      "1  0000AC05XPvAibaEXe9B2tolTZ0JLoBGbkQixQS6    1.0  2022.0   7.56    16.0   \n",
      "2  0001EB57XPvAibaEXe/twT+sf632fUXnsgPGeB4G    2.0  2019.0   9.00    21.0   \n",
      "3  0001EB57XPvAibaEXe/twT+sf632fUXnsgPGeB4G    1.0  2020.0   9.11    15.0   \n",
      "4  0001EB57XPvAibaEXe/twT+sf632fUXnsgPGeB4G    2.0  2020.0   8.75    19.0   \n",
      "\n",
      "        id   namsinh   gioitinh     noisinh      lopsh   khoahoc   tinhtrang  \\\n",
      "0      NaN       NaN        NaN         NaN        NaN       NaN         NaN   \n",
      "1      NaN       NaN        NaN         NaN        NaN       NaN         NaN   \n",
      "2  18570.0    2001.0        0.0   'Nghệ An'   TMĐT2019      14.0         1.0   \n",
      "3  18570.0    2001.0        0.0   'Nghệ An'   TMĐT2019      14.0         1.0   \n",
      "4  18570.0    2001.0        0.0   'Nghệ An'   TMĐT2019      14.0         1.0   \n",
      "\n",
      "     diachi_tinhtp Column1  dtbhk2   khoa_mahoa   hedt_mahoa  \\\n",
      "0              NaN     NaN    7.79          6.0          5.0   \n",
      "1              NaN     NaN    8.28          6.0          5.0   \n",
      "2   'Tỉnh Gia Lai'     NaN    8.84          1.0          2.0   \n",
      "3   'Tỉnh Gia Lai'     NaN    9.00          1.0          2.0   \n",
      "4   'Tỉnh Gia Lai'     NaN    9.11          1.0          2.0   \n",
      "\n",
      "    chuyennganh2_mahoa  \n",
      "0                 12.0  \n",
      "1                 12.0  \n",
      "2                  9.0  \n",
      "3                  9.0  \n",
      "4                  9.0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_data.csv\")\n",
    "ordinal_features = [' khoa', ' hedt', ' chuyennganh2']\n",
    "\n",
    "df[ordinal_features] = df[ordinal_features].fillna('Unknown')\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "df_encoded = encoder.fit_transform(df[ordinal_features])\n",
    "\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=[f\"{col}_mahoa\" for col in ordinal_features], index=df.index)\n",
    "\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "df.drop(columns=ordinal_features, inplace=True)\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.median(numeric_only=True))\n",
    "df = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "# Chọn biến đầu vào và đầu ra\n",
    "X = df.drop(columns=['nhom', 'dtbhk', 'id', ' namsinh', 'mssv', ' noisinh', ' diachi_tinhtp', 'Column1', ' lopsh'], errors='ignore')  # Xóa cột không cần thiết\n",
    "y = df['dtbhk']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.9144273316818844\n",
      "Mean Squared Error (MSE): 1.792692656017364\n",
      "Root Mean Squared Error (RMSE): 1.3389147306745728\n",
      "R^2 Score: 0.5175107420501133\n"
     ]
    }
   ],
   "source": [
    "lr_model.fit(X_train, y_train)\n",
    "# tập Test\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Đánh giá\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 22:10:44 WARN Utils: Your hostname, kpubuntu resolves to a loopback address: 127.0.1.1; using 10.0.240.155 instead (on interface wlp0s20f3)\n",
      "25/04/08 22:10:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/08 22:10:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 22:30:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentGradePrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"/DoAn/final_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Handle null values in ordinal features\n",
    "ordinal_features = [\" khoa\", \" hedt\", \" chuyennganh2\"]\n",
    "for col in ordinal_features:\n",
    "    df = df.na.fill(\"Unknown\", subset=[col])\n",
    "\n",
    "# Create string indexers for ordinal features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_mahoa\", handleInvalid=\"keep\")\n",
    "    for col in ordinal_features\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+-----+------+-------+--------+---------+----------+---------+-------+-------+--------+-------------+----------+---------------+-------+------+-----------+-----------+-------------------+\n",
      "|                mssv|hocky|namhoc|dtbhk|sotchk|     id| namsinh| gioitinh|   noisinh|    lopsh|   khoa|   hedt| khoahoc| chuyennganh2| tinhtrang|  diachi_tinhtp|Column1|dtbhk2| khoa_mahoa| hedt_mahoa| chuyennganh2_mahoa|\n",
      "+--------------------+-----+------+-----+------+-------+--------+---------+----------+---------+-------+-------+--------+-------------+----------+---------------+-------+------+-----------+-----------+-------------------+\n",
      "|0000AC05XPvAibaEX...|  2.0|2021.0| 8.28|  21.0|   NULL|    NULL|     NULL|      NULL|     NULL|Unknown|Unknown|    NULL|      Unknown|      NULL|           NULL|   NULL|  7.79|        3.0|        2.0|                1.0|\n",
      "|0000AC05XPvAibaEX...|  1.0|2022.0| 7.56|  16.0|   NULL|    NULL|     NULL|      NULL|     NULL|Unknown|Unknown|    NULL|      Unknown|      NULL|           NULL|   NULL|  8.28|        3.0|        2.0|                1.0|\n",
      "|0001EB57XPvAibaEX...|  2.0|2019.0|  9.0|  21.0|18570.0|  2001.0|      0.0| 'Nghệ An'| TMĐT2019|   HTTT|   CQUI|    14.0|    D52480104|       1.0| 'Tỉnh Gia Lai'|   NULL|  8.84|        1.0|        0.0|                8.0|\n",
      "|0001EB57XPvAibaEX...|  1.0|2020.0| 9.11|  15.0|18570.0|  2001.0|      0.0| 'Nghệ An'| TMĐT2019|   HTTT|   CQUI|    14.0|    D52480104|       1.0| 'Tỉnh Gia Lai'|   NULL|   9.0|        1.0|        0.0|                8.0|\n",
      "|0001EB57XPvAibaEX...|  2.0|2020.0| 8.75|  19.0|18570.0|  2001.0|      0.0| 'Nghệ An'| TMĐT2019|   HTTT|   CQUI|    14.0|    D52480104|       1.0| 'Tỉnh Gia Lai'|   NULL|  9.11|        1.0|        0.0|                8.0|\n",
      "+--------------------+-----+------+-----+------+-------+--------+---------+----------+---------+-------+-------+--------+-------------+----------+---------------+-------+------+-----------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['nhom', 'dtbhk', 'id', ' namsinh', 'mssv', ' noisinh', ' diachi_tinhtp', 'Column1', ' lopsh'] + ordinal_features\n",
    "\n",
    "# Define the pipeline stages\n",
    "pipeline_stages = indexers.copy()\n",
    "\n",
    "# Apply the pipeline to transform the data\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "transformed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Display the transformed data\n",
    "transformed_df.show(5)\n",
    "\n",
    "# Handle remaining nulls - Using median for numeric columns\n",
    "numeric_cols = [col for col in transformed_df.columns \n",
    "                if col not in columns_to_drop and transformed_df.schema[col].dataType.typeName() in ['integer', 'double']]\n",
    "\n",
    "# Calculate median for each numeric column and fill nulls\n",
    "medians = {}\n",
    "for col in numeric_cols:\n",
    "    median_value = transformed_df.approxQuantile(col, [0.5], 0.001)[0]\n",
    "    medians[col] = median_value\n",
    "    transformed_df = transformed_df.na.fill({col: median_value})\n",
    "\n",
    "# For remaining non-numeric columns, fill with mode\n",
    "for col in transformed_df.columns:\n",
    "    if col not in numeric_cols and col not in columns_to_drop:\n",
    "        # Find mode\n",
    "        mode_value = transformed_df.groupby(col).count().orderBy(\"count\", ascending=False).first()[0]\n",
    "        transformed_df = transformed_df.na.fill({col: mode_value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 22:31:14 WARN Instrumentation: [db087753] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 22:31:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/04/08 22:31:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.899235506863826\n",
      "Mean Squared Error (MSE): 1.7005400994995126\n",
      "Root Mean Squared Error (RMSE): 1.30404758329576\n",
      "R^2 Score: 0.5283404378295444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in transformed_df.columns \n",
    "                if col not in columns_to_drop]\n",
    "\n",
    "# Create vector assembler for feature columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(transformed_df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"dtbhk\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"dtbhk\", predictionCol=\"prediction\")\n",
    "\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Save the model\n",
    "lr_model.write().overwrite().save(\"spark_lr_model\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
